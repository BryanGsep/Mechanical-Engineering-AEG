{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('Data_Augment_Origin_2.csv')\n",
    "\n",
    "# Filter the DataFrame to include only the rows where the \"Score\" column is 0\n",
    "score_0_essays = df[df['Score'] == 0]['Essay'].tolist()\n",
    "score_1_essays = df[df['Score'] == 1]['Essay'].tolist()\n",
    "score_2_essays = df[df['Score'] == 2]['Essay'].tolist()\n",
    "\n",
    "origin_essays = [score_0_essays, score_1_essays, score_2_essays]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesized paraphase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dungbeoooiuuuthocuteephomaique/Desktop/Dung Folder/Nghien cuu khoa hoc/Dataset/Indonesia side/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "paraphrase_tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "def paraphrase(\n",
    "    question,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=5,\n",
    "    repetition_penalty=7.0,\n",
    "    diversity_penalty=2.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=64\n",
    "):\n",
    "    input_ids = paraphrase_tokenizer(\n",
    "        f'paraphrase: {question}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    \n",
    "    outputs = paraphrase_model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    res = paraphrase_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return res\n",
    "\n",
    "def generate_paraphrase(origin_text, return_num):\n",
    "    if return_num == 1:\n",
    "        return paraphrase(origin_text, num_beams=2, num_beam_groups=2, num_return_sequences=return_num)\n",
    "    else:\n",
    "        return paraphrase(origin_text, num_beams=return_num, num_beam_groups=return_num, num_return_sequences=return_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dungbeoooiuuuthocuteephomaique/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "SYNTHESIZED_NUMBER_PER_CLASS = 500\n",
    "\n",
    "synthesized_essays = [[], [], []]\n",
    "\n",
    "for i in range(3):\n",
    "    print(i)\n",
    "    return_num = math.ceil(SYNTHESIZED_NUMBER_PER_CLASS/len(origin_essays[i]))\n",
    "    for j in range(len(origin_essays[i])):\n",
    "        \n",
    "        sentences = sent_tokenize(origin_essays[i][j])\n",
    "        paraphrase_list = []\n",
    "        for sentence in sentences:\n",
    "            if len(paraphrase_list) == 0:\n",
    "                paraphrase_list = generate_paraphrase(sentence, return_num)\n",
    "            else:\n",
    "                paraphrase_list = [a + \" \" + b for a, b in zip(paraphrase_list, generate_paraphrase(sentence, return_num))]\n",
    "        paraphrase_and_origin_text = map(lambda x: [x, origin_essays[i][j]], paraphrase_list)\n",
    "        synthesized_essays[i] += paraphrase_and_origin_text\n",
    "    random.shuffle(synthesized_essays[i])\n",
    "    synthesized_essays[i] = synthesized_essays[i][:SYNTHESIZED_NUMBER_PER_CLASS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Check synthesized data length for each type of score\n",
    "\n",
    "print(len(synthesized_essays[0]))\n",
    "print(len(synthesized_essays[1]))\n",
    "print(len(synthesized_essays[2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune BERT as Baseline model to filter valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/dungbeoooiuuuthocuteephomaique/Desktop/Dung Folder/Nghien cuu khoa hoc/Dataset/Indonesia side/.venv/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, GPT2LMHeadModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_baseline = pd.read_csv('Data_Augment_Origin_2.csv', usecols=['Essay', 'Score'])\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encoding(texts):\n",
    "    return bert_tokenizer.batch_encode_plus(\n",
    "        texts.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Load the pre-trained BERT model and modify the final layer for classification\n",
    "baseline_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=3,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Set the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(baseline_model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Create a PyTorch DataLoader for the training and validation data\n",
    "class EssayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "length of train indices:  28\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dungbeoooiuuuthocuteephomaique/Desktop/Dung Folder/Nghien cuu khoa hoc/Dataset/Indonesia side/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.0996016561985016\n",
      "Validation Loss: 1.241928219795227\n",
      "Validation Accuracy: 0.125\n",
      "Epoch 2\n",
      "Training Loss: 0.99744513630867\n",
      "Validation Loss: 1.2876579761505127\n",
      "Validation Accuracy: 0.125\n",
      "Fold 2\n",
      "length of train indices:  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dungbeoooiuuuthocuteephomaique/Desktop/Dung Folder/Nghien cuu khoa hoc/Dataset/Indonesia side/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 1.0269542336463928\n",
      "Validation Loss: 0.881104052066803\n",
      "Validation Accuracy: 0.7142857313156128\n",
      "Epoch 2\n",
      "Training Loss: 0.9641855508089066\n",
      "Validation Loss: 0.9062682390213013\n",
      "Validation Accuracy: 0.7142857313156128\n",
      "Fold 3\n",
      "length of train indices:  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dungbeoooiuuuthocuteephomaique/Desktop/Dung Folder/Nghien cuu khoa hoc/Dataset/Indonesia side/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.9048813879489899\n",
      "Validation Loss: 0.7303051948547363\n",
      "Validation Accuracy: 1.0\n",
      "Epoch 2\n",
      "Training Loss: 0.8447778522968292\n",
      "Validation Loss: 0.7724858522415161\n",
      "Validation Accuracy: 0.8571428656578064\n",
      "Fold 4\n",
      "length of train indices:  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dungbeoooiuuuthocuteephomaique/Desktop/Dung Folder/Nghien cuu khoa hoc/Dataset/Indonesia side/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.8393738120794296\n",
      "Validation Loss: 0.6857599020004272\n",
      "Validation Accuracy: 1.0\n",
      "Epoch 2\n",
      "Training Loss: 0.6927273869514465\n",
      "Validation Loss: 0.5991281867027283\n",
      "Validation Accuracy: 1.0\n",
      "Fold 5\n",
      "length of train indices:  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dungbeoooiuuuthocuteephomaique/Desktop/Dung Folder/Nghien cuu khoa hoc/Dataset/Indonesia side/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.6535292416810989\n",
      "Validation Loss: 0.548404335975647\n",
      "Validation Accuracy: 1.0\n",
      "Epoch 2\n",
      "Training Loss: 0.57933659106493\n",
      "Validation Loss: 0.5044216513633728\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Initialize the cross-validator\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "# Iterate over the folds\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(df_train_baseline['Essay'])):\n",
    "    print(f'Fold {fold+1}')\n",
    "    print('length of train indices: ', len(train_indices))\n",
    "\n",
    "    # Initialize the data loaders for training and validation\n",
    "    df_train = df_train_baseline.loc[train_indices]\n",
    "    df_val = df_train_baseline.loc[val_indices]\n",
    "\n",
    "    train_encodings = encoding(df_train['Essay'])\n",
    "    val_encodings = encoding(df_val['Essay'])\n",
    "\n",
    "    train_dataset = EssayDataset(train_encodings, df_train['Score'].to_list())\n",
    "    val_dataset = EssayDataset(val_encodings, df_val['Score'].to_list())\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # Train and evaluate the model for each epoch\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        \n",
    "        # Train the model on the training data\n",
    "        baseline_model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = baseline_model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f'Training Loss: {train_loss}')\n",
    "        \n",
    "        # Evaluate the model on the validation data\n",
    "        baseline_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                outputs = baseline_model(**batch)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                val_loss += loss.item()\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_acc += (preds == batch['labels']).float().mean().item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "        print(f'Validation Loss: {val_loss}')\n",
    "        print(f'Validation Accuracy: {val_acc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use above baseline model to filter synthesized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baselineFilterData(essays, label_index,remain = 0.6):\n",
    "    baseline_model.eval()\n",
    "    essay_and_confident_score = []\n",
    "    with torch.no_grad():\n",
    "        for essay_and_origin in essays:\n",
    "            # Tokenize the essay and convert to input format\n",
    "            inputs = bert_tokenizer(essay_and_origin[0], truncation=True, max_length=512,return_tensors='pt')\n",
    "            outputs = baseline_model(**inputs)\n",
    "            # Get the predicted score\n",
    "            logits = outputs.logits\n",
    "            essay_and_confident_score.append([essay_and_origin[0], logits[0][label_index].item(), essay_and_origin[1]])\n",
    "    return sorted(essay_and_confident_score, key=lambda x: x[1], reverse=True)[:math.ceil(len(essay_and_confident_score)*remain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data and save as csv file\n",
    "baseline_filtered_data_0 = baselineFilterData(synthesized_essays[0], 0)\n",
    "baseline_filtered_data_1 = baselineFilterData(synthesized_essays[1], 1)\n",
    "baseline_filtered_data_2 = baselineFilterData(synthesized_essays[2], 2)\n",
    "\n",
    "baseline_filtered_data = [baseline_filtered_data_0, baseline_filtered_data_1, baseline_filtered_data_2]\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data = {'Essay': [], 'Score': [], 'Confident Score': [], 'Origin': []}\n",
    "\n",
    "# Loop through the essays and scores and append them to the dictionary\n",
    "for i in range(3):\n",
    "    data['Essay'].extend(map(lambda x: x[0], baseline_filtered_data[i]))\n",
    "    data['Confident Score'].extend(map(lambda x: x[1], baseline_filtered_data[i]))\n",
    "    data['Score'].extend([i] * len(baseline_filtered_data[i]))\n",
    "    data['Origin'].extend(map(lambda x: x[2], baseline_filtered_data[i]))\n",
    "\n",
    "# Create a Pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('Baseline_Filtered_Synthesized_Essays.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
