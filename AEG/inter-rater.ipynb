{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "def calculate_cohens_kappa(data_file, rater1, rater2):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # Step 2: Drop rows with empty elements in 'rater1' and 'rater2' columns\n",
    "    df = df.dropna(subset=[rater1, rater2])\n",
    "\n",
    "    # Assuming the column names for the scores are 'rater1' and 'rater2'\n",
    "    rater1_scores = df[rater1]\n",
    "    rater2_scores = df[rater2]\n",
    "\n",
    "    # Step 3: Calculate Cohen's Kappa\n",
    "    kappa_score = cohen_kappa_score(rater1_scores, rater2_scores, labels=[0, 1, 2])\n",
    "\n",
    "    return kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_tmp(data_file, rater1, rater2):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # Step 2: Drop rows with empty elements in 'rater1' and 'rater2' columns\n",
    "    df = df.dropna(subset=[rater1, rater2])\n",
    "\n",
    "    # Assuming the column names for the scores are 'rater1' and 'rater2'\n",
    "    rater1_scores = df[rater1]\n",
    "    rater2_scores = df[rater2]\n",
    "\n",
    "    # Step 3: Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(rater1_scores, rater2_scores)\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1.1 confusion matrix between rater 1 and rater 2:  [[8 0 0]\n",
      " [2 7 0]\n",
      " [0 6 6]]\n",
      "Q_2.1 confusion matrix between rater 1 and rater 2:  [[6 5 0]\n",
      " [1 9 4]\n",
      " [0 0 3]]\n",
      "Q_2.2 confusion matrix between rater 1 and rater 2:  [[6 3 0]\n",
      " [1 4 0]\n",
      " [0 2 6]]\n",
      "Q_1.1 confusion matrix between rater and model:  [[12  0  0]\n",
      " [ 1  9  1]\n",
      " [ 0  0 14]]\n",
      "Q_2.1 confusion matrix between rater and model:  [[13  0  0]\n",
      " [ 2 13  0]\n",
      " [ 0  0  8]]\n",
      "Q_2.2 confusion matrix between rater and model:  [[ 8  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  1 10]]\n"
     ]
    }
   ],
   "source": [
    "print('Q_1.1 confusion matrix between rater 1 and rater 2: ', confusion_matrix_tmp('./Question_1_1/Data_Augment_Origin.csv', 'Rater 1', 'Rater 2'))\n",
    "print('Q_2.1 confusion matrix between rater 1 and rater 2: ', confusion_matrix_tmp('./Question_2_1/Data_Augment_Origin_2.csv', 'Rater 1', 'Rater 2'))\n",
    "print('Q_2.2 confusion matrix between rater 1 and rater 2: ', confusion_matrix_tmp('./Question_2_2/Data_Augment_Origin_3.csv', 'Rater 1', 'Rater 2'))\n",
    "\n",
    "print('Q_1.1 confusion matrix between rater and model: ', confusion_matrix_tmp('./Question_1_1/Master_data_with_prediction.csv', 'Score', 'Predictions'))\n",
    "print('Q_2.1 confusion matrix between rater and model: ', confusion_matrix_tmp('./Question_2_1/Master_data_with_prediction_2_1_BERT.csv', 'Score', 'Predictions'))\n",
    "print('Q_2.2 confusion matrix between rater and model: ', confusion_matrix_tmp('./Question_2_2/Master_data_with_prediction_2_2_BERT.csv', 'Score', 'Predictions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1.1 Cohens kappa score between rater 1 and rater 2:  0.5944055944055944\n",
      "Q_2.1 Cohens kappa score between rater 1 and rater 2:  0.4285714285714286\n",
      "Q_2.2 Cohens kappa score between rater 1 and rater 2:  0.5975609756097562\n",
      "Q_1.1 Cohens kappa score between rater and model:  0.918141592920354\n",
      "Q_2.1 Cohens kappa score between rater and model:  0.9144893111638955\n",
      "Q_2.2 Cohens kappa score between rater and model:  0.947935368043088\n"
     ]
    }
   ],
   "source": [
    "print('Q_1.1 Cohens kappa score between rater 1 and rater 2: ', calculate_cohens_kappa('./Question_1_1/Data_Augment_Origin.csv', 'Rater 1', 'Rater 2'))\n",
    "print('Q_2.1 Cohens kappa score between rater 1 and rater 2: ', calculate_cohens_kappa('./Question_2_1/Data_Augment_Origin_2.csv', 'Rater 1', 'Rater 2'))\n",
    "print('Q_2.2 Cohens kappa score between rater 1 and rater 2: ', calculate_cohens_kappa('./Question_2_2/Data_Augment_Origin_3.csv', 'Rater 1', 'Rater 2'))\n",
    "\n",
    "print('Q_1.1 Cohens kappa score between rater and model: ', calculate_cohens_kappa('./Question_1_1/Master_data_with_prediction.csv', 'Score', 'Predictions'))\n",
    "print('Q_2.1 Cohens kappa score between rater and model: ', calculate_cohens_kappa('./Question_2_1/Master_data_with_prediction_2_1_BERT.csv', 'Score', 'Predictions'))\n",
    "print('Q_2.2 Cohens kappa score between rater and model: ', calculate_cohens_kappa('./Question_2_2/Master_data_with_prediction_2_2_BERT.csv', 'Score', 'Predictions'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
